# data_preprocessing.py
import pandas as pd
import numpy as np
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sentence_transformers import SentenceTransformer
import warnings
import pickle
warnings.filterwarnings('ignore')

class DataPreprocessor:
    def __init__(self):
        self.items_df = None
        self.train_df = None
        self.test_df = None
        self.text_embeddings = None
        self.tfidf_matrix_full = None
        self.tfidf_matrix_title = None
        self.svd_embeddings = None
        self.model = None
        self.model2 = None
        self.vectorizer_full = None
        self.vectorizer_title = None
        self.available_items = None
        self.relevance_pairs = None
        self.item_popularity = None
        self.query_expansion_dict = None

    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üöÄ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•...")
        self.items_df = pd.read_csv('items.csv')
        self.train_df = pd.read_csv('train.csv')
        self.test_df = pd.read_csv('test.csv')

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.items_df)} –æ–±—ä–µ–∫—Ç–æ–≤")
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.train_df)} –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä")
        print(f"üß™ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.test_df)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤")

        self.available_items = set(self.items_df['item_id'].values)
        self.build_enhanced_relevance_network()

    def build_enhanced_relevance_network(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        print("üï∏Ô∏è –ü–û–°–¢–†–û–ï–ù–ò–ï –°–ï–¢–ò –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò...")

        self.relevance_pairs = {}
        self.item_popularity = {}
        self.query_expansion_dict = {}

        # –ü—Ä—è–º—ã–µ —Å–≤—è–∑–∏
        for _, row in self.train_df.iterrows():
            query_id = row['query_id']
            item_id = row['item_id']

            if query_id in self.available_items and item_id in self.available_items:
                if query_id not in self.relevance_pairs:
                    self.relevance_pairs[query_id] = []
                self.relevance_pairs[query_id].append(item_id)
                self.item_popularity[item_id] = self.item_popularity.get(item_id, 0) + 1

        # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è expansion
        for query_id, items in self.relevance_pairs.items():
            for item_id in items:
                if item_id not in self.query_expansion_dict:
                    self.query_expansion_dict[item_id] = []
                self.query_expansion_dict[item_id].append(query_id)

        print(f"‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω–æ {len(self.relevance_pairs)} –ø—Ä—è–º—ã—Ö —Å–≤—è–∑–µ–π")
        print(f"‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω–æ {len(self.query_expansion_dict)} –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å–≤—è–∑–µ–π")

    def ultra_advanced_preprocess_text(self, text):
        """–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""

        text = str(text).lower()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        art_terms = {
            '–∏–º–ø—Ä–µ—Å—Å–∏–æ–Ω–∏–∑–º': 'ART_IMPRESSIONISM',
            '—ç–∫—Å–ø—Ä–µ—Å—Å–∏–æ–Ω–∏–∑–º': 'ART_EXPRESSIONISM',
            '—Ä–µ–Ω–µ—Å—Å–∞–Ω—Å': 'ART_RENAISSANCE',
            '–º–æ–¥–µ—Ä–Ω': 'ART_MODERN',
            '–∞–≤–∞–Ω–≥–∞—Ä–¥': 'ART_AVANTGARDE',
            '–∫–ª–∞—Å—Å–∏—Ü–∏–∑–º': 'ART_CLASSICISM',
            '—Ä–æ–º–∞–Ω—Ç–∏–∑–º': 'ART_ROMANTICISM',
            '—Ä–µ–∞–ª–∏–∑–º': 'ART_REALISM',
            '—Å–∏–º–≤–æ–ª–∏–∑–º': 'ART_SYMBOLISM',
            '–∫—É–±–∏–∑–º': 'ART_CUBISM'
        }

        for term, replacement in art_terms.items():
            text = text.replace(term, replacement)

        # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        text = re.sub(r'[^\w\s\-\.]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def prepare_maximum_features(self):
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üéØ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í...")

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤
        self.items_df['title'] = self.items_df['title'].fillna('')
        self.items_df['description'] = self.items_df['description'].fillna('')
        self.items_df['modality'] = self.items_df['modality'].fillna('unknown')
        self.items_df['extra'] = self.items_df['extra'].fillna('')

        # –£–ª—å—Ç—Ä–∞-–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        self.items_df['title_clean'] = self.items_df['title'].apply(self.ultra_advanced_preprocess_text)
        self.items_df['description_clean'] = self.items_df['description'].apply(self.ultra_advanced_preprocess_text)
        self.items_df['extra_clean'] = self.items_df['extra'].apply(self.ultra_advanced_preprocess_text)

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        self.items_df['combined_full'] = (
            self.items_df['title_clean'] + ' [SEP] ' +
            self.items_df['description_clean'] + ' [SEP] ' +
            self.items_df['extra_clean']
        )

        self.items_df['title_desc'] = self.items_df['title_clean'] + ' ' + self.items_df['description_clean']
        self.items_df['title_only'] = self.items_df['title_clean']

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        self.items_df['text_length'] = self.items_df['combined_full'].str.len()
        self.items_df['word_count'] = self.items_df['combined_full'].str.split().str.len()
        self.items_df['title_length'] = self.items_df['title_clean'].str.len()
        self.items_df['desc_length'] = self.items_df['description_clean'].str.len()
        self.items_df['word_density'] = self.items_df['word_count'] / (self.items_df['text_length'] + 1)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        self.items_df['unique_words'] = self.items_df['combined_full'].apply(
            lambda x: len(set(x.split())) if x else 0
        )

        initial_count = len(self.items_df)
        self.items_df = self.items_df[self.items_df['combined_full'].str.len() > 5]
        print(f"‚úÖ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(self.items_df)} –æ–±—ä–µ–∫—Ç–æ–≤")

        self.available_items = set(self.items_df['item_id'].values)

    def create_multi_model_embeddings(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        print("üîÆ –°–û–ó–î–ê–ù–ò–ï –ú–£–õ–¨–¢–ò-–ú–û–î–ï–õ–¨–ù–´–• –≠–ú–ë–ï–î–î–ò–ù–ì–û–í...")

        try:
            # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è diversity
            self.model2 = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')

            texts_full = self.items_df['combined_full'].tolist()
            texts_title = self.items_df['title_only'].tolist()

            print("üì• –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é...")
            self.text_embeddings_full = self.model.encode(texts_full, show_progress_bar=True)
            self.text_embeddings_title = self.model.encode(texts_title, show_progress_bar=False)

            print("üì• –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å—é...")
            self.text_embeddings_full2 = self.model2.encode(texts_full, show_progress_bar=False)

            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            self.text_embeddings = np.concatenate([
                self.text_embeddings_full,
                self.text_embeddings_full2
            ], axis=1)

            print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.text_embeddings.shape}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º—É–ª—å—Ç–∏-–º–æ–¥–µ–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            # Fallback –Ω–∞ –æ–¥–Ω—É –º–æ–¥–µ–ª—å
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            texts_full = self.items_df['combined_full'].tolist()
            self.text_embeddings = self.model.encode(texts_full, show_progress_bar=True)

    def create_advanced_tfidf(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üìä –°–û–ó–î–ê–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–´–• TF-IDF –ü–†–ò–ó–ù–ê–ö–û–í...")

        self.vectorizer_full = TfidfVectorizer(
            max_features=25000,
            min_df=1,
            max_df=0.75,
            ngram_range=(1, 4),  # –î–æ 4-–≥—Ä–∞–º–º
            stop_words=None,
            sublinear_tf=True,
            analyzer='word'
        )

        self.vectorizer_title = TfidfVectorizer(
            max_features=10000,
            min_df=1,
            max_df=0.8,
            ngram_range=(1, 3),
            stop_words=None,
            sublinear_tf=True
        )

        self.tfidf_matrix_full = self.vectorizer_full.fit_transform(self.items_df['combined_full'])
        self.tfidf_matrix_title = self.vectorizer_title.fit_transform(self.items_df['title_only'])

        # SVD –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        svd_full = TruncatedSVD(n_components=500, random_state=42, algorithm='arpack')
        svd_title = TruncatedSVD(n_components=300, random_state=42, algorithm='arpack')

        self.svd_embeddings_full = svd_full.fit_transform(self.tfidf_matrix_full)
        self.svd_embeddings_title = svd_title.fit_transform(self.tfidf_matrix_title)

        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º SVD —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.svd_embeddings = np.concatenate([
            self.svd_embeddings_full,
            self.svd_embeddings_title
        ], axis=1)

        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF full: {self.tfidf_matrix_full.shape}")
        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF title: {self.tfidf_matrix_title.shape}")
        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö SVD: {self.svd_embeddings.shape}")

    def save_preprocessed_data(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.items_df.to_csv('preprocessed_items.csv', index=False)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        with open('preprocessed_data.pkl', 'wb') as f:
            pickle.dump({
                'items_df': self.items_df,
                'text_embeddings': self.text_embeddings,
                'text_embeddings_title': self.text_embeddings_title,
                'svd_embeddings': self.svd_embeddings,
                'tfidf_matrix_full': self.tfidf_matrix_full,
                'tfidf_matrix_title': self.tfidf_matrix_title,
                'relevance_pairs': self.relevance_pairs,
                'item_popularity': self.item_popularity,
                'query_expansion_dict': self.query_expansion_dict,
                'available_items': self.available_items,
                'train_df': self.train_df,
                'test_df': self.test_df,
                'vectorizer_full': self.vectorizer_full,
                'vectorizer_title': self.vectorizer_title
            }, f)

        print("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ preprocessed_data.pkl –∏ preprocessed_items.csv")

def preprocess_main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    try:
        print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•")
        print("="*60)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        preprocessor = DataPreprocessor()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        preprocessor.load_data()

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        preprocessor.prepare_maximum_features()

        # –°–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏-–º–æ–¥–µ–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        preprocessor.create_multi_model_embeddings()

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        preprocessor.create_advanced_tfidf()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        preprocessor.save_preprocessed_data()

        print("\nüéâ –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏")

    except Exception as e:
        print(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    preprocess_main()
