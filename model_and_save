# model_ranking.py
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import CrossEncoder
import pickle
import warnings
warnings.filterwarnings('ignore')

class RankingModel:
    def __init__(self):
        self.items_df = None
        self.train_df = None
        self.test_df = None
        self.text_embeddings = None
        self.title_embeddings = None
        self.svd_embeddings = None
        self.tfidf_matrix_full = None
        self.tfidf_matrix_title = None
        self.cross_encoder = None
        self.reranker = None
        self.vectorizer_full = None
        self.vectorizer_title = None
        self.available_items = None
        self.relevance_pairs = None
        self.item_popularity = None
        self.query_expansion_dict = None
        self.item_similarity_matrix = None

    def load_preprocessed_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("üìÇ –ó–ê–ì–†–£–ó–ö–ê –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•...")

        with open('preprocessed_data.pkl', 'rb') as f:
            data = pickle.load(f)

        self.items_df = data['items_df']
        self.text_embeddings = data['text_embeddings']
        self.text_embeddings_title = data['text_embeddings_title']
        self.svd_embeddings = data['svd_embeddings']
        self.tfidf_matrix_full = data['tfidf_matrix_full']
        self.tfidf_matrix_title = data['tfidf_matrix_title']
        self.relevance_pairs = data['relevance_pairs']
        self.item_popularity = data['item_popularity']
        self.query_expansion_dict = data['query_expansion_dict']
        self.available_items = data['available_items']
        self.train_df = data['train_df']
        self.test_df = data['test_df']
        self.vectorizer_full = data['vectorizer_full']
        self.vectorizer_title = data['vectorizer_title']

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.items_df)} –æ–±—ä–µ–∫—Ç–æ–≤")
        print(f"üß™ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.test_df)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤")

    def initialize_advanced_rerankers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–æ–≤"""
        print("üéØ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ü–ï–†–ï–†–ê–ù–ñ–ò–†–û–í–©–ò–ö–û–í...")

        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä
            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            print("‚úÖ –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫
            try:
                self.reranker = CrossEncoder('cross-encoder/stsb-roberta-base')
                print("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –∑–∞–≥—Ä—É–∂–µ–Ω")
            except:
                print("‚ö†Ô∏è  –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π")
                self.reranker = self.cross_encoder

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–æ–≤: {e}")
            print("‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–æ–≤")
            self.cross_encoder = None
            self.reranker = None

    def build_mega_similarity_matrix(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–µ–≥–∞-–º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        print("üèóÔ∏è –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–ï–ì–ê-–ú–ê–¢–†–ò–¶–´ –°–•–û–ñ–ï–°–¢–ò...")

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏
        embedding_similarity = cosine_similarity(self.text_embeddings)
        title_similarity = cosine_similarity(self.text_embeddings_title)
        svd_similarity = cosine_similarity(self.svd_embeddings)

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.item_similarity_matrix = (
            0.50 * embedding_similarity +
            0.30 * title_similarity +
            0.20 * svd_similarity
        )

        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ–≥–∞-–º–∞—Ç—Ä–∏—Ü—ã: {self.item_similarity_matrix.shape}")

    def query_expansion(self, query_item_id):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ train –¥–∞–Ω–Ω—ã—Ö"""
        expanded_queries = []

        if query_item_id in self.relevance_pairs:
            # –ü—Ä—è–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
            for relevant_item in self.relevance_pairs[query_item_id]:
                if relevant_item in self.query_expansion_dict:
                    expanded_queries.extend(self.query_expansion_dict[relevant_item])

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        expanded_queries = list(set(expanded_queries))
        return expanded_queries[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

    def get_candidate_pool(self, query_item_id, num_candidates=200):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            if query_item_id not in self.available_items:
                return []

            query_idx = self.items_df[self.items_df['item_id'] == query_item_id].index[0]
            query_modality = self.items_df.loc[query_idx, 'modality']

            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
            similarities = self.item_similarity_matrix[query_idx]

            candidate_scores = []
            for idx, score in enumerate(similarities):
                if idx == query_idx:
                    continue

                candidate_id = self.items_df.iloc[idx]['item_id']
                candidate_modality = self.items_df.iloc[idx]['modality']

                if candidate_modality != query_modality and score > 0.05:
                    popularity = self.item_popularity.get(candidate_id, 0)
                    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
                    combined_score = score + (np.log1p(popularity) * 0.1)
                    candidate_scores.append((candidate_id, combined_score, candidate_modality))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Ç–±–∏—Ä–∞–µ–º
            candidate_scores.sort(key=lambda x: x[1], reverse=True)

            # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
            expansion_candidates = []
            expanded_queries = self.query_expansion(query_item_id)

            for exp_query in expanded_queries:
                if exp_query in self.available_items:
                    exp_idx = self.items_df[self.items_df['item_id'] == exp_query].index[0]
                    exp_similarities = self.item_similarity_matrix[exp_idx]

                    for idx, score in enumerate(exp_similarities):
                        if idx == exp_idx:
                            continue

                        candidate_id = self.items_df.iloc[idx]['item_id']
                        candidate_modality = self.items_df.iloc[idx]['modality']

                        if (candidate_modality != query_modality and
                            candidate_id != query_item_id and
                            score > 0.1):

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —ç—Ç–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                            existing = any(c[0] == candidate_id for c in candidate_scores[:num_candidates//2])
                            if not existing:
                                expansion_candidates.append((candidate_id, score * 0.8, candidate_modality))

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            all_candidates = candidate_scores[:num_candidates//2] + expansion_candidates

            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
            unique_candidates = {}
            for candidate_id, score, modality in all_candidates:
                if candidate_id not in unique_candidates:
                    unique_candidates[candidate_id] = (score, modality)
                else:
                    # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
                    unique_candidates[candidate_id] = (max(unique_candidates[candidate_id][0], score), modality)

            return [(candidate_id, score, modality)
                   for candidate_id, (score, modality) in unique_candidates.items()]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {e}")
            return []

    def advanced_reranking(self, query_item_id, candidates, top_k=10):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        try:
            if not candidates or not self.cross_encoder:
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ ID –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                return [(c[0], c[1], c[2]) for c in candidates[:top_k]]

            query_idx = self.items_df[self.items_df['item_id'] == query_item_id].index[0]
            query_text = self.items_df.loc[query_idx, 'combined_full']

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidate_texts = []
            candidate_data = []

            for candidate_id, score, modality in candidates[:50]:  # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ç–æ–ø-50
                candidate_idx = self.items_df[self.items_df['item_id'] == candidate_id].index[0]
                candidate_text = self.items_df.loc[candidate_idx, 'combined_full']
                candidate_texts.append(candidate_text)
                candidate_data.append((candidate_id, score, modality))

            # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä—ã –æ—Ç –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–∞
            pairs = [(query_text, cand_text) for cand_text in candidate_texts]
            cross_scores = self.cross_encoder.predict(pairs)

            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å–∫–æ—Ä—ã
            reranked_candidates = []
            for i, (candidate_id, semantic_score, modality) in enumerate(candidate_data):
                cross_score = cross_scores[i]

                # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
                if semantic_score > 0.3:  # –í—ã—Å–æ–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    final_score = 0.4 * semantic_score + 0.6 * cross_score
                else:  # –°—Ä–µ–¥–Ω–µ–µ/–Ω–∏–∑–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    final_score = 0.2 * semantic_score + 0.8 * cross_score

                # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
                popularity_bonus = np.log1p(self.item_popularity.get(candidate_id, 0)) * 0.05
                final_score += popularity_bonus

                reranked_candidates.append((candidate_id, final_score, modality))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É
            reranked_candidates.sort(key=lambda x: x[1], reverse=True)

            # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä—Ç–µ–∂–∏ –∏–∑ 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            return reranked_candidates[:top_k]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            return [(c[0], c[1], c[2]) for c in candidates[:top_k]]

    def diversity_optimization(self, candidates, top_k=10):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        if not candidates:
            return []

        final_recommendations = []
        modality_counts = {}
        used_items = set()

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        for candidate_id, score, modality in candidates:
            if len(final_recommendations) >= top_k:
                break

            if candidate_id in used_items:
                continue

            # –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π
            current_count = modality_counts.get(modality, 0)
            max_per_modality = max(3, top_k // 3)  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç

            if current_count < max_per_modality:
                final_recommendations.append(candidate_id)
                used_items.add(candidate_id)
                modality_counts[modality] = current_count + 1

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏—Ö –∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
        if len(final_recommendations) < top_k:
            for candidate_id, score, modality in candidates:
                if len(final_recommendations) >= top_k:
                    break
                if candidate_id not in used_items:
                    final_recommendations.append(candidate_id)
                    used_items.add(candidate_id)

        return final_recommendations[:top_k]

    def get_maximum_ndcg_recommendations(self, query_item_id, top_k=10):
        """–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –í–û–ó–ú–û–ñ–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï nDCG"""
        try:
            if query_item_id not in self.available_items:
                fallback = self.get_ultimate_fallback(top_k)
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                return [item[0] if isinstance(item, tuple) else item for item in fallback]

            # –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidates = self.get_candidate_pool(query_item_id, num_candidates=200)

            if not candidates:
                fallback = self.get_ultimate_fallback(top_k)
                return [item[0] if isinstance(item, tuple) else item for item in fallback]

            # –≠—Ç–∞–ø 2: –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            reranked_candidates = self.advanced_reranking(query_item_id, candidates, top_k)

            # –≠—Ç–∞–ø 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            final_recommendations = self.diversity_optimization(reranked_candidates, top_k)

            # –≠—Ç–∞–ø 4: –ì–∞—Ä–∞–Ω—Ç–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            if len(final_recommendations) < min(7, top_k):
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                if query_item_id in self.relevance_pairs:
                    train_items = [item for item in self.relevance_pairs[query_item_id]
                                 if item not in final_recommendations and item in self.available_items]
                    final_recommendations.extend(train_items[:top_k - len(final_recommendations)])

            return final_recommendations[:top_k]

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {query_item_id}: {e}")
            fallback = self.get_ultimate_fallback(top_k)
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            return [item[0] if isinstance(item, tuple) else item for item in fallback]

    def get_ultimate_fallback(self, top_k=10):
        """–£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç"""
        try:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            modality_items = {}

            for item_id in self.available_items:
                modality = self.items_df[self.items_df['item_id'] == item_id]['modality'].iloc[0]
                popularity = self.item_popularity.get(item_id, 0)

                if modality not in modality_items:
                    modality_items[modality] = []
                modality_items[modality].append((item_id, popularity))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            for modality in modality_items:
                modality_items[modality].sort(key=lambda x: x[1], reverse=True)

            # –°–æ–±–∏—Ä–∞–µ–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            recommendations = []
            max_per_modality = max(2, top_k // len(modality_items))

            while len(recommendations) < top_k:
                for modality in list(modality_items.keys()):
                    if len(recommendations) >= top_k:
                        break
                    if modality_items[modality]:
                        item_id, popularity = modality_items[modality][0]
                        recommendations.append(item_id)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ ID
                        modality_items[modality].pop(0)
                    else:
                        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏
                        del modality_items[modality]

                if not modality_items:
                    break

            return recommendations[:top_k]

        except:
            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π fallback
            all_items = list(self.available_items)
            return all_items[:top_k]

    def generate_maximum_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"""
        print("üéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ì–û –ö–ê–ß–ï–°–¢–í–ê...")

        recommendations = []

        for i, query_item_id in enumerate(self.test_df['id']):
            if i % 10 == 0:
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(self.test_df)} –∑–∞–ø—Ä–æ—Å–æ–≤")

            max_ndcg_items = self.get_maximum_ndcg_recommendations(query_item_id, top_k=10)
            recommendations.append(' '.join(max_ndcg_items))

        return recommendations

    def create_final_submission(self, recommendations):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ submission —Ñ–∞–π–ª–∞"""
        print("üíæ –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û SUBMISSION...")

        submission_df = self.test_df.copy()
        submission_df['relevant_item_id_list'] = recommendations
        submission_df.to_csv('submission.csv', index=False)

        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
        rec_lengths = [len(rec.split()) for rec in recommendations]

        print("\n" + "="*50)
        print("üéâ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*50)
        print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(recommendations)}")
        print(f"üìà –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {np.mean(rec_lengths):.2f}")
        print(f"üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {min(rec_lengths)}")
        print(f"üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {max(rec_lengths)}")

        # –î–µ—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        length_counts = pd.Series(rec_lengths).value_counts().sort_index()
        print("\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
        for length, count in length_counts.items():
            percentage = count/len(recommendations)*100
            print(f"   {length} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {count} –∑–∞–ø—Ä–æ—Å–æ–≤ ({percentage:.1f}%)")

        # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        ten_recommendations = rec_lengths.count(10)
        print(f"\nüéØ –ó–∞–ø—Ä–æ—Å–æ–≤ —Å 10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏: {ten_recommendations} ({ten_recommendations/len(recommendations)*100:.1f}%)")

        high_quality = sum(1 for length in rec_lengths if length >= 8)
        print(f"üèÜ –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (8+): {high_quality} ({high_quality/len(recommendations)*100:.1f}%)")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        print("üöÄ –ó–ê–ü–£–°–ö –ú–û–î–ï–õ–ò –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø")
        print("="*60)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        ranking_model = RankingModel()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        ranking_model.load_preprocessed_data()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–æ–≤
        ranking_model.initialize_advanced_rerankers()

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–µ–≥–∞-–º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏
        ranking_model.build_mega_similarity_matrix()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        print("\nüéØ –ù–ê–ß–ê–õ–û –ì–ï–ù–ï–†–ê–¶–ò–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô...")
        recommendations = ranking_model.generate_maximum_recommendations()

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ submission
        ranking_model.create_final_submission(recommendations)

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª—É—á—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        print("\n‚≠ê –õ–£–ß–®–ò–ï –ü–†–ò–ú–ï–†–´ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:")
        print("="*50)

        displayed = 0
        for i in range(min(5, len(ranking_model.test_df))):
            query_id = ranking_model.test_df.iloc[i]['id']
            if query_id in ranking_model.available_items:
                query_item = ranking_model.items_df[ranking_model.items_df['item_id'] == query_id].iloc[0]
                recommended = recommendations[i].split()

                print(f"\nüîç –ó–∞–ø—Ä–æ—Å: {query_item['title']}")
                print(f"   –ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å: {query_item['modality']}")
                print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(recommended)}")
                print("   –¢–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")

                for j, rec_id in enumerate(recommended[:8]):
                    if rec_id in ranking_model.available_items:
                        rec_item = ranking_model.items_df[ranking_model.items_df['item_id'] == rec_id].iloc[0]
                        print(f"   {j+1}. {rec_item['title'][:45]}... ({rec_item['modality']})")

                displayed += 1
                if displayed >= 3:
                    break

        print("\nüéâ –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ submission.csv")

    except Exception as e:
        print(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
